# -*- coding: utf-8 -*-
"""sentimental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1efa3MuIeWsWZTZnF9UY09k0ZvyxI76aY
"""

#Install required libraries
!pip install -q transformers datasets scikit-learn

# Import Libraries
import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
from torch.utils.data import Dataset

#  Load Data
train_df = pd.read_csv("/content/drive/MyDrive/test/train_2kmZucJ.csv")
test_df = pd.read_csv("/content/drive/MyDrive/test/test_oJQbWVk.csv")
sample_submission = pd.read_csv("/content/drive/MyDrive/test/sample_submission_LnhVWA4.csv")

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)
train_df.head()

# Label distribution
print(train_df['label'].value_counts())

#Tokenization
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class TweetDataset(Dataset):
    def __init__(self, texts, labels=None):
        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128)
        self.labels = labels.tolist() if labels is not None else None

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

#  Train/Validation Split
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_df["tweet"], train_df["label"], test_size=0.1, stratify=train_df["label"], random_state=42
)

train_dataset = TweetDataset(train_texts, train_labels)
val_dataset = TweetDataset(val_texts, val_labels)

#  Load Model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

## ✅ Training Arguments (fast config)
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="no",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    weight_decay=0.01,
    logging_steps=10,
    report_to=[]  # disables W&B and other loggers
)



# ✅ Evaluation Metric
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    f1 = f1_score(labels, preds, average="weighted")
    return {"f1": f1}

# ✅ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

import os
os.environ["WANDB_DISABLED"] = "true"

# ✅ Train
trainer.train()

# ✅ Inference on Test Set
test_dataset = TweetDataset(test_df["tweet"])
preds_output = trainer.predict(test_dataset)
test_preds = np.argmax(preds_output.predictions, axis=1)

# ✅ Prepare Submission
submission = sample_submission.copy()
submission["label"] = test_preds
submission.to_csv("submission.csv", index=False)

print("✅ Submission saved to 'submission.csv'")